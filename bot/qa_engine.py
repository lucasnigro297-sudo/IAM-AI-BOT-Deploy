# bot/qa_engine.py
# === Orquestador: memoria + (opcional) RAG + LLM ===

from __future__ import annotations
from typing import List, Optional

# LangChain opcional/tolerante: si falta, el bot sigue sin RAG
try:
    from langchain_community.vectorstores import FAISS as LCFAISS
    from langchain_community.embeddings import HuggingFaceEmbeddings
except Exception as e:
    LCFAISS = None
    HuggingFaceEmbeddings = None
    print(f"‚ö†Ô∏è LangChain community no disponible: {e}")

from .memory import ConversationMemory
from bot.llm_client import generate  # funci√≥n que llama a tu modelo (Groq/Ollama/etc.)

# ---------- RAG (opcional): embeddings + carga de √≠ndice FAISS ----------
_embeddings = None
if HuggingFaceEmbeddings is not None:
    try:
        _embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo inicializar HuggingFaceEmbeddings: {e}")
        _embeddings = None

vectorstore = None
if LCFAISS is not None and _embeddings is not None:
    try:
        # Carga del √≠ndice local generado previamente (faiss_index/)
        vectorstore = LCFAISS.load_local(
            "faiss_index",
            _embeddings,
            allow_dangerous_deserialization=True,
        )
        print("‚úÖ Vectorstore (RAG) cargado.")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo cargar faiss_index: {e}")
        vectorstore = None
else:
    print("‚ÑπÔ∏è RAG deshabilitado (faltan dependencias o embeddings).")

# ---------- Memoria conversacional por sesi√≥n (global en el proceso) ----------
memory = ConversationMemory(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    device="cpu",
)

# ---------- Helpers ----------
def _buscar_documentos_relacionados(pregunta: str, k: int = 4) -> List[str]:
    """Devuelve textos de los k chunks m√°s similares si RAG est√° activo; si no, []."""
    if not vectorstore:
        return []
    try:
        documentos = vectorstore.similarity_search(pregunta, k=k)
        print("\nüìö Chunks utilizados para responder:")
        resumenes = []
        for i, doc in enumerate(documentos):
            fuente = doc.metadata.get("source", "Desconocida")
            pagina = doc.metadata.get("page", "N/A")
            inicio = (doc.page_content or "")[:200].replace("\n", " ")
            print(f"\nüîπ Chunk {i + 1} (Fuente: {fuente}, P√°gina: {pagina}): {inicio}...")
            resumenes.append(doc.page_content or "")
        return resumenes
    except Exception as e:
        print(f"‚ö†Ô∏è Error en similarity_search(): {e}")
        return []


def _construir_prompt(contexto_memoria: str, contexto_docs: str, pregunta: str) -> str:
    """Estructura del prompt final: instrucciones + memoria + docs + pregunta."""
    instrucciones = (
        "Sos un asistente especializado en Identity & Access Management (IAM). "
        "Respond√© en espa√±ol, con precisi√≥n y de forma clara. "
        "No digas que ‚Äòya se respondi√≥ antes‚Äô; aunque se repita, respond√© de nuevo con s√≠ntesis. "
        "Si la respuesta no est√° en el contexto, reconocelo y propon√© una pr√≥xima pregunta.\n"
    )
    partes = [instrucciones]
    if contexto_memoria:
        partes.append(f"=== CONTEXTO DE CONVERSACI√ìN ===\n{contexto_memoria.strip()}")
    if contexto_docs:
        partes.append(f"=== CONTEXTO DE DOCUMENTACI√ìN ===\n{contexto_docs.strip()}")
    partes.append(f"=== PREGUNTA ===\n{pregunta}\n=== RESPUESTA ===")
    return "\n\n".join(partes)

# ---------- Punto de entrada: responde a una pregunta ----------
def responder(pregunta: str, session_id: Optional[str]) -> str:
    """
    Flujo:
      1) Construir contexto con memoria (ANTES de guardar el turno actual).
      2) Agregar contexto de RAG si disponible.
      3) Llamar al LLM con prompt final.
      4) Guardar (user/assistant) en memoria (DESPU√âS).
    """
    try:
        sid = session_id or "default"

        # 1) Contexto de memoria
        system_hint = (
            "Us√° la memoria de la conversaci√≥n solo para dar continuidad, no inventes datos. "
            "Prioriz√° la documentaci√≥n t√©cnica si hay contradicci√≥n."
        )
        try:
            contexto_memoria = memory.build_context(
                session_id=sid, query=pregunta, k=6, system_hint=system_hint, recent_k=6
            )
        except Exception as e:
            print(f"‚ö†Ô∏è build_context() fall√≥: {e}")
            contexto_memoria = system_hint

        # 2) Contexto de documentaci√≥n (RAG)
        docs = _buscar_documentos_relacionados(pregunta, k=4)
        contexto_docs = "\n\n".join(docs) if docs else ""

        # 3) Prompt final y llamado al LLM
        prompt = _construir_prompt(contexto_memoria, contexto_docs, pregunta)
        try:
            respuesta = (generate(prompt, system="Asistente IAM") or "").strip()
        except Exception as e:
            print(f"‚ùå Error llamando al LLM: {e}")
            respuesta = ""
        if not respuesta:
            respuesta = "No pude generar una respuesta con el contexto disponible."

        # 4) Persistencia en memoria del turno actual
        try:
            memory.add_message(sid, role="user", content=pregunta)
            memory.add_message(sid, role="assistant", content=respuesta)
            print(f"üß† Guardado turno: sesi√≥n={sid} total_msgs={len(memory._sessions.get(sid).texts)}")
        except Exception as e:
            print(f"‚ö†Ô∏è add_message fall√≥: {e}")

        return respuesta

    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"‚ùå Error al procesar la respuesta: {str(e)}"


__all__ = ["responder", "memory"]
