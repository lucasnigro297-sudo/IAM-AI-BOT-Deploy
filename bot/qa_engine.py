# bot/qa_engine.py
from __future__ import annotations
from typing import List, Optional

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

from .memory import ConversationMemory
from bot.llm_client import generate  # üëà ahora usamos SIEMPRE esto

# -------------------------------------------------
# Cargamos VectorDB global (documentaci√≥n / RAG)
# -------------------------------------------------
_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

try:
    vectorstore = FAISS.load_local(
        "faiss_index",
        _embeddings,
        allow_dangerous_deserialization=True
    )
except Exception as e:
    print(f"‚ö†Ô∏è No se pudo cargar faiss_index: {e}")
    vectorstore = None

# -------------------------------------------------
# Memoria conversacional por sesi√≥n (en RAM)
# -------------------------------------------------
memory = ConversationMemory(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    device="cpu"
)

# -------------------------------------------------
# Helpers
# -------------------------------------------------
def _buscar_documentos_relacionados(pregunta: str, k: int = 4) -> List[str]:
    if not vectorstore:
        return []
    documentos = vectorstore.similarity_search(pregunta, k=k)

    print("\nüìö Chunks utilizados para responder:")
    resumenes = []
    for i, doc in enumerate(documentos):
        fuente = doc.metadata.get("source", "Desconocida")
        pagina = doc.metadata.get("page", "N/A")
        inicio = doc.page_content[:200].replace("\n", " ")
        print(f"\nüîπ Chunk {i + 1} (Fuente: {fuente}, P√°gina: {pagina}): {inicio}...")
        resumenes.append(doc.page_content)
    return resumenes


def _construir_prompt(contexto_memoria: str, contexto_docs: str, pregunta: str) -> str:
    instrucciones = (
        "Sos un asistente especializado en Identity & Access Management (IAM). "
        "Respond√© en espa√±ol, con precisi√≥n y de forma clara. "
        "Si la respuesta no est√° en el contexto, reconocelo y propon√© una pr√≥xima pregunta.\n"
    )

    partes = [instrucciones]

    if contexto_memoria:
        partes.append(f"=== CONTEXTO DE CONVERSACI√ìN ===\n{contexto_memoria.strip()}")

    if contexto_docs:
        partes.append(f"=== CONTEXTO DE DOCUMENTACI√ìN ===\n{contexto_docs.strip()}")

    partes.append(f"=== PREGUNTA ===\n{pregunta}\n=== RESPUESTA ===")

    return "\n\n".join(partes)

# -------------------------------------------------
# Funci√≥n principal
# -------------------------------------------------
def responder(pregunta: str, session_id: Optional[str]) -> str:
    try:
        # 1) Guardar mensaje del usuario
        memory.add_message(session_id, role="user", content=pregunta)

        # 2) Contexto de memoria
        system_hint = (
            "Us√° la memoria de la conversaci√≥n solo para dar continuidad, no inventes datos. "
            "Prioriz√° la documentaci√≥n t√©cnica si hay contradicci√≥n."
        )
        contexto_memoria = memory.build_context(
            session_id=session_id,
            query=pregunta,
            k=6,
            system_hint=system_hint
        )

        # 3) RAG de documentos
        docs = _buscar_documentos_relacionados(pregunta, k=4)
        contexto_docs = "\n\n".join(docs)

        # 4) Prompt final
        prompt = _construir_prompt(contexto_memoria, contexto_docs, pregunta)

        # 5) Llamar al LLM v√≠a `llm_client.generate()`
        respuesta = (generate(prompt, system="Asistente IAM") or "").strip()
        if not respuesta:
            respuesta = "No pude generar una respuesta con el contexto disponible."

        # 6) Guardar respuesta en memoria
        memory.add_message(session_id, role="assistant", content=respuesta)
        return respuesta

    except Exception as e:
        print(f"‚ùå Error en responder(): {e}")
        return f"‚ùå Error al procesar la respuesta: {str(e)}"
