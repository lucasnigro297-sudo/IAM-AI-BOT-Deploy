# ğŸ¤– Bot Conversacional IAM - AI Challenge

## ğŸ“Œ DescripciÃ³n
Este proyecto implementa un bot conversacional para el equipo de **Identity and Access Management (IAM)** que permite realizar consultas sobre documentos cargados, con el objetivo de aprender mÃ¡s rÃ¡pido sobre conceptos y tendencias en la protecciÃ³n de cuentas de usuario.  
Se utiliza un **LLM Open Source** (LLaMA 3 vÃ­a Ollama) y un flujo **RAG** con FAISS para bÃºsqueda de informaciÃ³n relevante.

---

## ğŸ“‚ Estructura del Proyecto
```
IAM-AI-BOT/
â”œâ”€ bot/               # LÃ³gica del bot y motor de preguntas (qa_engine.py)
â”œâ”€ data/documentos    # Documentos fuente para el RAG
â”œâ”€ faiss_index/       # Ãndice generado con FAISS (se crea al cargar documentos)
â”œâ”€ frontend/          # Interfaz web tipo ChatGPT usando chat-ui
â”œâ”€ main.py            # API en FastAPI
â”œâ”€ requirements.txt   # Dependencias Python
â””â”€ README.md          # Este archivo
```

---

## ğŸš€ EjecuciÃ³n del Proyecto

### 1ï¸âƒ£ Requisitos previos
- Python **3.10+**
- Node.js **20+** (para el frontend)
- [Ollama](https://ollama.ai) instalado y corriendo
- Git (si se clona desde repositorio)

---

### 2ï¸âƒ£ Backend (FastAPI + RAG)
```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate   # En Linux/Mac
venv\Scripts\activate      # En Windows

# Instalar dependencias
pip install -r requirements.txt

# Iniciar backend
uvicorn main:app --reload
```
El backend quedarÃ¡ disponible en:
```
http://localhost:8000/docs
```

---

### 3ï¸âƒ£ Frontend (chat-ui + Vite)
```bash
cd frontend

# Instalar dependencias
npm install

# Levantar entorno de desarrollo
npm run dev
```
El frontend quedarÃ¡ disponible en:
```
http://localhost:5173
```

---

## ğŸ›  Decisiones de DiseÃ±o
- **FastAPI** para el backend: ligero, rÃ¡pido y fÃ¡cil de documentar.
- **RAG con FAISS**: mejora la precisiÃ³n recuperando chunks relevantes antes de pasarlos al modelo.
- **LLaMA 3 vÃ­a Ollama**: modelo open source y local, cumpliendo la consigna de no usar memoria interna del LLM.
- **Frontend con chat-ui**: para simular la experiencia de ChatGPT, cumpliendo el requisito visual.
- **UUID por sesiÃ³n**: persistencia de contexto a nivel navegador sin usar almacenamiento del modelo.
- **Memoria en API**: las conversaciones se guardan temporalmente en un diccionario de Python usando el UUID de la sesiÃ³n.

---

## ğŸ§ª Pruebas Conversacionales
### Escenarios probados:
1. **Preguntas simples**: â€œÂ¿QuÃ© es IAM?â€ â†’ respuesta correcta desde documentos.

![alt text](image-4.png)

2. **Preguntas de seguridad**: â€œÂ¿Por quÃ© no es recomendable un token de sesiÃ³n con expiraciÃ³n grande?â€ â†’ explicaciÃ³n con riesgos.

![alt text](image-5.png)

3. **Preguntas fuera de contexto**: devuelve mensaje de no encontrar informaciÃ³n relevante.

![alt text](image-6.png)

4. **Persistencia de contexto**: mantener coherencia en preguntas encadenadas en la misma sesiÃ³n.

![alt text](image-7.png)


### Resultados:
- **PrecisiÃ³n**: Responde correctamente en base a los documentos cargados.
- **Velocidad**: Promedio < 5 segundos por respuesta en entorno local.
- **UX**: Interfaz limpia, similar a ChatGPT, con indicador de escritura.

---

## ğŸ“‹ Estado de la Consigna

âœ… **Obligatorio**  
- LLM open source (LLaMA 3)  
- API en Python (FastAPI)  
- Respuestas basadas en documentos (RAG con FAISS)  
- Memoria por sesiÃ³n (UUID en frontend)  
- Frontend similar a ChatGPT (chat-ui)  
- Repositorio privado (Github)

âš ï¸ **Opcional Implementado**  
- Persistencia de sesiones entre reinicios del frontend  
- DocumentaciÃ³n tÃ©cnica (este README)  

âŒ **Opcional Pendiente**  
- Despliegue en la nube (ej. Render, Railway, etc.)  

---

## ğŸ“Š DocumentaciÃ³n tÃ©cnica
- [Arquitectura general](docs/arquitectura.md)
- [Flujo de RAG](docs/flujo-rag.md)
- [Secuencia de un turno de chat](docs/secuencia-chat.md)
- [Componentes backend](./componentes.md) <!-- opcional -->
- [Despliegue (opcional)](docs/despliegue.md)

---

## ğŸ‘¨â€ğŸ’» Autor
Desarrollado por Lucas Ivan Nigro como parte de **MeLi Challenge**.